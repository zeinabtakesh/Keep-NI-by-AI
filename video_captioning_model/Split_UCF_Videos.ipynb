{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11371728,"sourceType":"datasetVersion","datasetId":7118824},{"sourceId":11371892,"sourceType":"datasetVersion","datasetId":7119091}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **NoteBook Summary**\n\nThis notebook presents a complete pipeline for segmenting long videos into shorter, captioned clips based on timestamp annotations. Using OpenCV, the notebook first loads each video and extracts key properties such as frame rate (fps), resolution, and total frame count. It then converts each annotated timestamp pair (start and end times in seconds) into corresponding frame indices. To ensure that each generated clip contains a meaningful visual segment, the notebook enforces a minimum of 8 frames per clipâ€”automatically extending the segment forward or backward if needed without exceeding the video bounds. For each segment, frames are extracted and saved into a new video file using cv2.VideoWriter. A CSV file is generated to map each clip's file path to its corresponding caption, enabling easy reference for training or evaluation in downstream tasks. Finally, the notebook includes steps to organize and upload the resulting dataset to Kaggle, including metadata creation and publishing through the Kaggle API.\n\n","metadata":{}},{"cell_type":"markdown","source":"A. Video Loading and Property Extraction\nOpenCV Video Capture:\nThe code uses cv2.VideoCapture to open the original video file. The properties such as fps, total frame count, frame width, and height are obtained with cap.get() calls.\nCitation: OpenCV VideoCapture Documentation\n\nB. Frame Conversion\nTimestamps to Frame Indices:\nThe timestamp (seconds) is multiplied by the fps to yield frame indices (e.g., start_frame = int(start_time * fps)). This conversion is standard in video processing.\n\nC. Ensuring Minimum Number of Frames\nFrame Deficit Handling:\nThe code checks if the number of frames in the segment is fewer than 8. If so, it first attempts to extend the segment forward. If that is not sufficient (e.g., reaching the end of the video), it then extends backward. This ensures that each clip contains at least eight frames.\n\nD. Video Clipping and Writing\nWriting Video Clips:\nA loop extracts frames starting from the adjusted start_frame to end_frame and writes them into a new video file using cv2.VideoWriter.\nCitation: OpenCV VideoWriter Documentation\n\nE. CSV File Creation\nMapping Video Path and Caption:\nThe mapping is stored in a list and then written into a CSV file with pandas. This CSV serves as an index connecting each generated clip to its corresponding caption.\n\n4. Useful Links and Citations\nOpenCV Documentation:\n\nVideo Capture and Video Properties: OpenCV Documentation\n\nVideo Writer: OpenCV VideoWriter Documentation\n\nPython JSON Library: Python JSON Docs\n\npandas: pandas Documentation","metadata":{"id":"dQBsLxLFOJFO"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport json\nimport os\nimport math\nimport pandas as pd\n\n# --- Configuration ---\njson_path = \"/kaggle/input/ucf-mini/normal_sampled_annotations.json\"\nvideos_dir = \"/kaggle/input/ucf-mini\"\noutput_dir = \"./splitted_clips_normal\"\ncsv_output_path = \"./splitted_clips_normal.csv\"\nmin_frames = 8\n\n# Create output directory if it does not exist\nos.makedirs(output_dir, exist_ok=True)\n\n# --- Load JSON Dataset ---\nwith open(json_path, \"r\") as f:\n    dataset = json.load(f)\n\ncsv_rows = []\n\n# --- Process Each Video in the Dataset ---\nfor video_name, video_data in dataset.items():\n    video_file = os.path.join(videos_dir, video_name + \".mp4\")  # Adjust the extension if necessary\n\n    # Open video capture using OpenCV\n    cap = cv2.VideoCapture(video_file)\n    if not cap.isOpened():\n        print(f\"Error opening video {video_file}\")\n        continue\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # or use 'XVID' if you prefer\n\n    # Process each segment: iterate over timestamps and corresponding caption\n    for idx, (times, caption) in enumerate(zip(video_data[\"timestamps\"], video_data[\"sentences\"])):\n        start_time, end_time = times\n        start_frame = int(start_time * fps)\n        end_frame = int(end_time * fps)\n        num_frames = end_frame - start_frame\n\n        # Adjust segment if less than minimum required frames\n        if num_frames < min_frames:\n            deficit = min_frames - num_frames\n\n            # Try extending forward\n            if end_frame + deficit <= total_frames:\n                end_frame += deficit\n            else:\n                # If not enough frames ahead, try extending backward\n                if start_frame - deficit >= 0:\n                    start_frame -= deficit\n                else:\n                    # Fallback: use entire range if possible\n                    start_frame = 0\n                    end_frame = total_frames\n\n            # Update num_frames based on new indices\n            num_frames = end_frame - start_frame\n\n        # Set the video to the starting frame\n        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n\n        # Prepare output clip file path and writer\n        output_clip_path = os.path.join(f\"{video_name}_clip{idx}.mp4\")\n        out = cv2.VideoWriter(output_clip_path, fourcc, fps, (width, height))\n\n        # Write frames to the new clip\n        frames_written = 0\n        for f in range(start_frame, end_frame):\n            ret, frame = cap.read()\n            if not ret:\n                break\n            out.write(frame)\n            frames_written += 1\n        out.release()\n\n        print(f\"Extracted clip: {output_clip_path} with {frames_written} frames.\")\n\n        # Append CSV mapping info: ensuring the caption is trimmed\n        csv_rows.append({\n            \"video_path\": output_clip_path,\n            \"caption\": caption.strip()\n        })\n\n    # Reset cap to the beginning (or close and reopen for next video)\n    cap.release()\n\n# --- Save CSV Mapping ---\ndf = pd.DataFrame(csv_rows)\ndf.to_csv(csv_output_path, index=False)\nprint(f\"CSV mapping saved to: {csv_output_path}\")\n","metadata":{"id":"9uw7bFN-OFNO","outputId":"2748b36d-a540-41b3-8ebc-e04dc4e1ed03","trusted":true,"execution":{"iopub.status.busy":"2025-04-12T11:42:41.830744Z","iopub.execute_input":"2025-04-12T11:42:41.831089Z","execution_failed":"2025-04-12T11:41:51.273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Save CSV Mapping ---\ndf = pd.DataFrame(csv_rows)\ndf.to_csv(\"/kaggle/working/splitted_clips_normal/splitted_clips_normal.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T11:39:54.439225Z","iopub.execute_input":"2025-04-12T11:39:54.43978Z","iopub.status.idle":"2025-04-12T11:39:54.450318Z","shell.execute_reply.started":"2025-04-12T11:39:54.439752Z","shell.execute_reply":"2025-04-12T11:39:54.449515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport os\n\n# Save kaggle.json in ~/.kaggle/\nwith open('/kaggle/working/kaggle.json', 'w') as f:\n    json.dump({\"username\":\"nourfakih\",\"key\":\"0005ac45aa3dc353c01a5d486ed0a5ac\"}, f)\n\nos.environ['KAGGLE_CONFIG_DIR'] = \"/kaggle/working\"\n\n!mkdir -p ~/.kaggle\n!cp /kaggle/working/kaggle.json ~/.kaggle/\n!chmod 600 ~/.kaggle/kaggle.json\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T11:41:16.603152Z","iopub.execute_input":"2025-04-12T11:41:16.603468Z","iopub.status.idle":"2025-04-12T11:41:16.981289Z","shell.execute_reply.started":"2025-04-12T11:41:16.603443Z","shell.execute_reply":"2025-04-12T11:41:16.980217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\ndataset_name = \"splitted-clips-normal\"\n\n# os.makedirs('/kaggle/working/' + dataset_name, exist_ok=True)\n\n# # Move the folder inside the dataset directory\n# shutil.move(\"/kaggle/working/UCF-mini-dataset/sampled_annotations.json\", \"/kaggle/working/UCF-mini-dataset/{dataset_name}\")\n\n# Create the metadata file\nmetadata = f\"\"\"\n{{\n  \"title\": \"{dataset_name}\",\n  \"id\": \"nourfakih/{dataset_name}\",\n  \"licenses\": [{{\"name\": \"CC0-1.0\"}}]\n}}\n\"\"\"\n\nwith open(\"/kaggle/working/splitted_clips_normal/dataset-metadata.json\", \"w\") as f:\n    f.write(metadata)\n","metadata":{"id":"HRofNwabQpeW","trusted":true,"execution":{"iopub.status.busy":"2025-04-12T11:41:20.983863Z","iopub.execute_input":"2025-04-12T11:41:20.984193Z","iopub.status.idle":"2025-04-12T11:41:20.99047Z","shell.execute_reply.started":"2025-04-12T11:41:20.984166Z","shell.execute_reply":"2025-04-12T11:41:20.989613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle datasets create -p /kaggle/working/splitted_clips_normal \n","metadata":{"id":"-Pg1aLlRQrPS","trusted":true,"execution":{"iopub.status.busy":"2025-04-12T11:41:23.071179Z","iopub.execute_input":"2025-04-12T11:41:23.071565Z","iopub.status.idle":"2025-04-12T11:42:09.236117Z","shell.execute_reply.started":"2025-04-12T11:41:23.071536Z","shell.execute_reply":"2025-04-12T11:42:09.235018Z"}},"outputs":[],"execution_count":null}]}
