{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11354553,"sourceType":"datasetVersion","datasetId":7105581},{"sourceId":11423193,"sourceType":"datasetVersion","datasetId":7154027}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **NoteBook Summary:**\n\nThis notebook presents a complete pipeline for training and evaluating a video captioning model based on a VisionEncoderDecoder architecture that combines a TimeSformer encoder with a GPT-2 decoder. The workflow begins by loading and optionally splitting a dataset of captioned video clips into train, validation, and test sets using group-based sampling. Videos are processed using the av library to extract a fixed number of frames per clip, which are then transformed into tensors using a pretrained video image processor. Text captions are tokenized using GPT-2’s tokenizer. The notebook builds custom PyTorch Dataset and DataLoader classes, defines a training loop with AMP (automatic mixed precision), and evaluates performance using the ROUGE metric. Throughout training, checkpoints are saved locally and pushed to the Hugging Face Hub via Git. Finally, the notebook compares caption generation between the custom-trained model and an existing pretrained model (Neleac/timesformer-gpt2-video-captioning) across a set of unseen video clips, storing both outputs in a CSV file for inspection. The setup supports reproducible, scalable, and Hub-integrated model training and deployment.","metadata":{}},{"cell_type":"code","source":"!pip install av datasets\n!pip install transformers torch\n!pip install rouge_score\n!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:25:11.018199Z","iopub.execute_input":"2025-04-20T11:25:11.018553Z","iopub.status.idle":"2025-04-20T11:25:23.467710Z","shell.execute_reply.started":"2025-04-20T11:25:11.018525Z","shell.execute_reply":"2025-04-20T11:25:23.466916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport multiprocessing\nimport os\n\nimport av\nfrom datasets import load_dataset\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoImageProcessor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:25:23.469471Z","iopub.execute_input":"2025-04-20T11:25:23.469742Z","iopub.status.idle":"2025-04-20T11:25:23.474181Z","shell.execute_reply.started":"2025-04-20T11:25:23.469719Z","shell.execute_reply":"2025-04-20T11:25:23.473436Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Login to Huggingface","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()  # will prompt you for your HF token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:25:23.474953Z","iopub.execute_input":"2025-04-20T11:25:23.475161Z","iopub.status.idle":"2025-04-20T11:25:23.504357Z","shell.execute_reply.started":"2025-04-20T11:25:23.475137Z","shell.execute_reply":"2025-04-20T11:25:23.503577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n\n# # Load Hugging Face token from Kaggle secret\n# hf_token = os.environ.get(\"HF_TOKEN\")\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n\n# Use it in your login\nfrom huggingface_hub import login\nlogin(token=secret_value_0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:25:23.506154Z","iopub.execute_input":"2025-04-20T11:25:23.506360Z","iopub.status.idle":"2025-04-20T11:25:23.769281Z","shell.execute_reply.started":"2025-04-20T11:25:23.506345Z","shell.execute_reply":"2025-04-20T11:25:23.768518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # prompt: convert csv to df\n\n# import pandas as pd\n\n# # Assuming your CSV file is named 'your_file.csv' and is in the current directory.\n# # Replace 'your_file.csv' with the actual path to your CSV file if it's located elsewhere.\n# df = pd.read_csv('/kaggle/input/splitted-ucf-120videospercategory/splitted-ucf-clips.csv')\n# # Remove \"./splitted_clips/\" from the 'video_path' column\n# df['video_path'] = df['video_path'].str.replace('./splitted_clips/', '')\n# # Display the first few rows of the DataFrame to verify\n# df\n# import pandas as pd\n# from sklearn.model_selection import GroupShuffleSplit\n# from datasets import Dataset\n\n# # Assuming 'df' is your DataFrame and it includes a 'video_path' column\n# df['video_group'] = df['video_path'].str.split('_').str[0]  # Adjust based on actual naming\n\n# # Step 1: Train-Test split (85% train+val, 15% test)\n# gss1 = GroupShuffleSplit(test_size=0.05, n_splits=1, random_state=42)\n# train_val_inds, test_inds = next(gss1.split(df, groups=df['video_group']))\n# train_val_df = df.iloc[train_val_inds].reset_index(drop=True)\n# test_df = df.iloc[test_inds].reset_index(drop=True)\n\n# # Step 2: Train-Validation split (from 85% of original data → 70% train, 15% val overall)\n# gss2 = GroupShuffleSplit(test_size=0.1, n_splits=1, random_state=42)  \n# # 0.1765 ≈ 15 / 85 → ensures ~15% of full data goes to validation\n# train_inds, val_inds = next(gss2.split(train_val_df, groups=train_val_df['video_group']))\n# train_df = train_val_df.iloc[train_inds].reset_index(drop=True)\n# valid_df = train_val_df.iloc[val_inds].reset_index(drop=True)\n\n# # Shuffle for randomness\n# train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n# valid_df = valid_df.sample(frac=1, random_state=42).reset_index(drop=True)\n# test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# # Convert to Hugging Face Datasets format\n# dataset = {\n#     \"train\": Dataset.from_pandas(train_df[:50]),\n#     \"validation\": Dataset.from_pandas(valid_df[:10]),\n#     \"test\": Dataset.from_pandas(test_df[:10])\n# }\n\n# # Display sizes\n# print(f\"Train: {len(train_df)}, Validation: {len(valid_df)}, Test: {len(test_df)}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:25:23.770149Z","iopub.execute_input":"2025-04-20T11:25:23.770400Z","iopub.status.idle":"2025-04-20T11:25:23.774468Z","shell.execute_reply.started":"2025-04-20T11:25:23.770381Z","shell.execute_reply":"2025-04-20T11:25:23.773753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from datasets import DatasetDict\n# image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n# tokenizer.pad_token = tokenizer.eos_token\n\n# # create dataset with features: videoID, pixel_values (8, 3, 224, 224), labels (10, 1024)\n# def process(example):\n#     videoID, captions = example[\"video_path\"], example[\"caption\"]\n    \n#     videos_path = \"/kaggle/input/splitted-ucf-120videospercategory/splitted-ucf-clips\"\n#     video_path = os.path.join(videos_path, videoID)\n#     if not os.path.isfile(video_path):\n#         video_path = os.path.join(videos_path, videoID)\n#     container = av.open(video_path)\n    \n#     # discrepancy between in codec metadata, manually get frame count\n#     container.seek(0)\n#     frame_count = 0\n#     for frame in container.decode(video=0):\n#         frame_count += 1\n    \n#     indices = set(np.linspace(0, frame_count, num=8, endpoint=False).astype(np.int64))\n#     frames = []\n#     container.seek(0)\n#     for i, frame in enumerate(container.decode(video=0)):\n#         if i in indices:\n#             frames.append(frame.to_ndarray(format=\"rgb24\"))   \n            \n#     pixel_values = image_processor(frames).pixel_values[0]\n#     labels = tokenizer(captions, padding=\"max_length\").input_ids\n#     return {\"videoID\": videoID, \"pixel_values\": pixel_values, \"labels\": labels}\n\n# dataset[\"train\"] = dataset[\"train\"].map(\n#     process,\n#     remove_columns=[\"caption\"],\n#     num_proc=1,\n#     cache_file_name=\"/kaggle/tmp/dataset-ucf/cache\",    # ← write here\n#     writer_batch_size=500                          # flush every 500 examples\n# )\n# dataset[\"validation\"] = dataset[\"validation\"].map(process, remove_columns=[\"caption\"], num_proc=1)\n# dataset[\"test\"] = dataset[\"test\"].map(process, remove_columns=[\"caption\"], num_proc=1)\n# from datasets import DatasetDict\n\n# # Wrap your dict into a DatasetDict\n# dataset_dict = DatasetDict({\n#     \"train\": dataset[\"train\"],\n#     \"validation\": dataset[\"validation\"],\n#     \"test\": dataset[\"test\"]\n    \n# })\n# from datasets import DatasetDict\n# dataset = DatasetDict(dataset)\n# #dataset.save_to_disk(\"./dataset-ucf\", num_proc=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:25:23.775326Z","iopub.execute_input":"2025-04-20T11:25:23.775581Z","iopub.status.idle":"2025-04-20T11:25:23.792212Z","shell.execute_reply.started":"2025-04-20T11:25:23.775558Z","shell.execute_reply":"2025-04-20T11:25:23.791620Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Mapped Data from Huggingface","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# 1. Load all splits at once\ndataset = load_dataset(\"NourFakih/ucf101-captioned-mapped\")  \n# `dataset` is a DatasetDict with keys like \"train\", \"validation\", \"test\" :contentReference[oaicite:0]{index=0}\n\ntrain_ds      = dataset[\"train\"]\nvalidation_ds = dataset[\"validation\"]\ntest_ds       = dataset[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:25:23.792984Z","iopub.execute_input":"2025-04-20T11:25:23.793227Z","iopub.status.idle":"2025-04-20T11:32:14.057476Z","shell.execute_reply.started":"2025-04-20T11:25:23.793210Z","shell.execute_reply":"2025-04-20T11:32:14.056656Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Base model","metadata":{}},{"cell_type":"code","source":"import json\nimport os\n\nfrom datasets import Dataset, load_from_disk\nimport numpy as np\nimport torch\nfrom torch.cuda.amp import GradScaler\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm.auto import tqdm\nfrom transformers import VisionEncoderDecoderModel, AutoImageProcessor, AutoTokenizer, default_data_collator, get_scheduler\n\ndevice = \"cuda\"\n\n# MODEL\nencoder = \"facebook/timesformer-base-finetuned-k600\"\ndecoder = \"gpt2\"\n\nimage_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\ntokenizer = AutoTokenizer.from_pretrained(decoder)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel_name=\"Neleac/SpaceTimeGPT\"\nmodel = VisionEncoderDecoderModel.from_pretrained(model_name).to(device)\n#model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(encoder, decoder).to(device)\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.max_length = 50\nmodel.config.num_beams = 4\nmodel.config.early_stopping = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:32:14.058371Z","iopub.execute_input":"2025-04-20T11:32:14.058661Z","iopub.status.idle":"2025-04-20T11:32:30.074358Z","shell.execute_reply.started":"2025-04-20T11:32:14.058633Z","shell.execute_reply":"2025-04-20T11:32:30.073533Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prepare Data for training","metadata":{}},{"cell_type":"code","source":"# ───────────────────────────────────────────────────────────────────────────────\n# Disable tokenizers parallelism to avoid deadlocks\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Imports\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import VisionEncoderDecoderModel , get_scheduler\nfrom transformers import default_data_collator\nfrom torch.amp import autocast, GradScaler\nfrom datasets import load_from_disk\n\n# ───────────────────────────────────────────────────────────────────────────────","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:32:30.075314Z","iopub.execute_input":"2025-04-20T11:32:30.075591Z","iopub.status.idle":"2025-04-20T11:32:30.080415Z","shell.execute_reply.started":"2025-04-20T11:32:30.075567Z","shell.execute_reply":"2025-04-20T11:32:30.079900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ───────────────────────────────────────────────────────────────────────────────\n# Disable tokenizers parallelism to avoid deadlocks\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Imports\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import VisionEncoderDecoderModel , get_scheduler\nfrom transformers import default_data_collator\nfrom torch.amp import autocast, GradScaler\nfrom datasets import load_from_disk\n\n# # ───────────────────────────────────────────────────────────────────────────────\n# 1. Dataset definition and loading\nclass UCFDataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n         example = self.dataset[idx]\n         label = example[\"labels\"]\n         if isinstance(label, list):\n                  label = label[0]\n         return {\n            \"video_path\":   example[\"video_path\"],\n            \"pixel_values\": example[\"pixel_values\"],\n            \"labels\":  label     \n         }\n     \n\n\n# load from disk and set PyTorch format\n#dataset = load_from_disk(\"./dataset-ucf\")\ndataset.set_format(\"torch\")\ndataset_train = UCFDataset(dataset[\"train\"])\ndataset_val   = UCFDataset(dataset[\"validation\"])\n\n# ───────────────────────────────────────────────────────────────────────────────\n# 2. Collators to ensure correct tensor shapes\ndef train_collator(examples):\n    pixel_values = torch.stack([ex[\"pixel_values\"] for ex in examples])\n    labels       = torch.stack([ex[\"labels\"]       for ex in examples])\n    return {\"pixel_values\": pixel_values, \"labels\": labels}\n\ndef val_collator(examples):\n    pixel_values = torch.stack([ex[\"pixel_values\"] for ex in examples])\n    labels       = torch.stack([ex[\"labels\"]       for ex in examples])\n    video_paths  = [ex[\"video_path\"] for ex in examples]\n    return {\"pixel_values\": pixel_values, \"labels\": labels, \"video_paths\": video_paths}\n\n# ───────────────────────────────────────────────────────────────────────────────\n# 3. DataLoaders\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_dataloader = DataLoader(\n    dataset_train,\n    collate_fn = train_collator,\n    batch_size  = 1,\n    drop_last   = True,\n    num_workers = 1,\n    pin_memory  = True,\n)\nval_dataloader = DataLoader(\n    dataset_val,\n    collate_fn = val_collator,\n    batch_size  = 1,\n    drop_last   = False,\n    num_workers = 1,\n    pin_memory  = True,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:32:30.082803Z","iopub.execute_input":"2025-04-20T11:32:30.082997Z","iopub.status.idle":"2025-04-20T11:32:33.067906Z","shell.execute_reply.started":"2025-04-20T11:32:30.082982Z","shell.execute_reply":"2025-04-20T11:32:33.067086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ───────────────────────────────────────────────────────────────────────────────\n# 4. Model, optimizer, scheduler, and AMP scaler\nmodel     = VisionEncoderDecoderModel.from_pretrained(\"Neleac/SpaceTimeGPT\").to(device)\noptimizer = AdamW(model.parameters(), lr=5e-7)\n#optimizer = AdamW(model.parameters(), lr=1e-6)\nepochs    = 4\ntotal_steps = epochs * len(train_dataloader)\nscheduler   = get_scheduler(\n    name             = \"linear\",\n    optimizer        = optimizer,\n    num_warmup_steps = 0,\n    num_training_steps = total_steps,\n)\nscaler = GradScaler(\"cuda\")  # :contentReference[oaicite:0]{index=0}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:32:33.068682Z","iopub.execute_input":"2025-04-20T11:32:33.068900Z","iopub.status.idle":"2025-04-20T11:32:34.531514Z","shell.execute_reply.started":"2025-04-20T11:32:33.068884Z","shell.execute_reply":"2025-04-20T11:32:34.530730Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"####  Video Captioning Training","metadata":{}},{"cell_type":"code","source":"# from tqdm.auto import tqdm  # auto handles Jupyter vs script\n\n# # Training loop with visible tqdm\n# for epoch in range(epochs):\n#     model.train()\n#     train_loss = 0.0\n#     loop = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} - Training\", leave=True)\n    \n#     for batch in loop:\n#         pixel_values = batch[\"pixel_values\"].to(device)\n#         labels = batch[\"labels\"]\n#         if labels.dim() == 1:\n#             labels = labels.unsqueeze(0)\n#         labels = labels.to(device)\n\n#         optimizer.zero_grad()\n#         with autocast(device_type=\"cuda\", dtype=torch.float16):\n#             outputs = model(pixel_values=pixel_values, labels=labels)\n#             loss = outputs.loss\n#             train_loss += loss.item()\n\n#         scaler.scale(loss).backward()\n#         scaler.step(optimizer)\n#         scaler.update()\n#         scheduler.step()\n\n#         loop.set_postfix(loss=loss.item())  # update progress bar with current loss\n\n#     # clear unused GPU memory to reduce fragmentation :contentReference[oaicite:2]{index=2}\n#     torch.cuda.empty_cache()\n\n#     # --- Validation ---\n#     model.eval()\n#     val_loss = 0.0\n#     with torch.no_grad():\n#         for batch in val_dataloader:\n#             pixel_values = batch[\"pixel_values\"].to(device)\n#             labels       = batch[\"labels\"]\n#             if labels.dim() == 1:\n#                 labels = labels.unsqueeze(0)\n#             labels = labels.to(device)\n\n#             with autocast(device_type=\"cuda\", dtype=torch.float16):\n#                 outputs = model(pixel_values=pixel_values, labels=labels)\n#                 val_loss += outputs.loss.item()\n\n#     # logging\n#     avg_train = train_loss / len(train_dataloader)\n#     avg_val   = val_loss   / len(val_dataloader)\n#     print(f\"Epoch {epoch+1} — train_loss: {avg_train:.4f}, val_loss: {avg_val:.4f}\")\n\n#     # save checkpoint\n#     model.save_pretrained(f\"./training-ucf/checkpoint_{epoch+1}\")\n# # ───────────────────────────────────────────────────────────────────────────────","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:32:34.532490Z","iopub.execute_input":"2025-04-20T11:32:34.532784Z","iopub.status.idle":"2025-04-20T11:32:34.537544Z","shell.execute_reply.started":"2025-04-20T11:32:34.532760Z","shell.execute_reply":"2025-04-20T11:32:34.536839Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  TimeSformer+GPT2 Training and Validation with Rouge Metric","metadata":{}},{"cell_type":"code","source":"# !pip install rouge_score\n# !pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:32:34.538475Z","iopub.execute_input":"2025-04-20T11:32:34.538760Z","iopub.status.idle":"2025-04-20T11:32:34.553375Z","shell.execute_reply.started":"2025-04-20T11:32:34.538719Z","shell.execute_reply":"2025-04-20T11:32:34.552821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import shutil\n# import torch\n# import numpy as np\n# import nltk\n# from tqdm.auto import tqdm\n# from huggingface_hub import Repository\n# import evaluate\n\n# # ─── Setup Rouge ───────────────────────────────────────────────────────────────\n# nltk.download(\"punkt\")\n# rouge = evaluate.load(\"rouge\")  # Load ROUGE metric :contentReference[oaicite:0]{index=0}\n\n# def postprocess_text(preds, labels):\n#     preds = [p.strip() for p in preds]\n#     labels = [l.strip() for l in labels]\n#     preds = [\"\\n\".join(nltk.sent_tokenize(p)) for p in preds]\n#     labels = [\"\\n\".join(nltk.sent_tokenize(l)) for l in labels]\n#     return preds, labels\n\n# def compute_metrics(eval_preds):\n#     preds, labels = eval_preds\n#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n#     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n#     result = rouge.compute(predictions=decoded_preds,\n#                            references=decoded_labels,\n#                            use_stemmer=True)\n#     # scale to percentages\n#     return {k: round(v * 100, 4) for k, v in result.items()}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:32:34.554086Z","iopub.execute_input":"2025-04-20T11:32:34.554324Z","iopub.status.idle":"2025-04-20T11:32:34.566882Z","shell.execute_reply.started":"2025-04-20T11:32:34.554300Z","shell.execute_reply":"2025-04-20T11:32:34.566303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# ─── Prepare HF Hub Repo ────────────────────────────────────────────────────\n# Clone (or create) a HF repo into \"./training-ucf\"\n# repo = Repository(\n#     local_dir=\"training-ucf\",\n#     clone_from=\"NourFakih/TimeSformer-GPT2-UCF-7000\",\n#     use_auth_token=True\n# )  # uses git under the hood :contentReference[oaicite:1]{index=1}\n\nfrom huggingface_hub import HfApi, Repository  # install via pip install huggingface_hub\n\n# 1) Create an empty model repo on HF (won’t error if it already exists)\napi = HfApi()\napi.create_repo(\n    repo_id=\"NourFakih/TimeSformer-GPT2-UCF-8380\",\n    repo_type=\"model\",\n    exist_ok=True\n)  # :contentReference[oaicite:0]{index=0}\n\n# 2) Now clone that empty repo into your local folder\nrepo = Repository(\n    local_dir=\"training-ucf\",                          # where on disk to put it\n    clone_from=\"NourFakih/TimeSformer-GPT2-UCF-8380\",   # the HF namespace/repo\n    use_auth_token=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:32:34.568276Z","iopub.execute_input":"2025-04-20T11:32:34.568472Z","iopub.status.idle":"2025-04-20T11:32:34.915360Z","shell.execute_reply.started":"2025-04-20T11:32:34.568456Z","shell.execute_reply":"2025-04-20T11:32:34.914810Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ─── Training Loop ─────────────────────────────────────────────────────────────\n# for epoch in range(epochs):\n#     # — Training —\n#     model.train()\n#     train_loss = 0.0\n#     pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} • Training\", leave=True)\n#     for batch in pbar:\n#         pixel_values = batch[\"pixel_values\"].to(device)\n#         labels = batch[\"labels\"]\n#         if labels.dim() == 1: labels = labels.unsqueeze(0)\n#         labels = labels.to(device)\n\n#         optimizer.zero_grad()\n#         with autocast(device_type=\"cuda\", dtype=torch.float16):\n#             outputs = model(pixel_values=pixel_values, labels=labels)\n#             loss = outputs.loss\n#             train_loss += loss.item()\n#         scaler.scale(loss).backward()\n#         scaler.step(optimizer)\n#         scaler.update()\n#         scheduler.step()\n\n#         pbar.set_postfix(loss=loss.item())\n\n#     torch.cuda.empty_cache()\n\n#     # — Validation & ROUGE —\n#     model.eval()\n#     val_loss = 0.0\n#     all_preds, all_labels = [], []\n#     with torch.no_grad():\n#         for batch in val_dataloader:\n#             pixel_values = batch[\"pixel_values\"].to(device)\n#             labels = batch[\"labels\"]\n#             if labels.dim() == 1: labels = labels.unsqueeze(0)\n#             labels = labels.to(device)\n\n#             # 1) compute loss\n#             with autocast(device_type=\"cuda\", dtype=torch.float16):\n#                 outputs = model(pixel_values=pixel_values, labels=labels)\n#                 val_loss += outputs.loss.item()\n\n#             # 2) generate predictions for ROUGE\n#             generated_ids = model.generate(\n#                 pixel_values,\n#                 max_length=labels.shape[-1],       # adjust as needed\n#                 num_beams=4,                       # optional\n#                 early_stopping=True\n#             )\n#             all_preds.extend(generated_ids.cpu().numpy())\n#             all_labels.extend(labels.cpu().numpy())\n\n#     avg_train = train_loss / len(train_dataloader)\n#     avg_val   = val_loss   / len(val_dataloader)\n#     rouge_scores = compute_metrics((np.array(all_preds), np.array(all_labels)))\n#     print(f\"Epoch {epoch+1} — train_loss: {avg_train:.4f}, \"\n#           f\"val_loss: {avg_val:.4f}, ROUGE: {rouge_scores}\")\n\n#     # — Keep only latest checkpoint locally —\n#     prev_ckpt = f\"./training-ucf/checkpoint_{epoch}\"\n#     if epoch > 0 and os.path.isdir(prev_ckpt):\n#         shutil.rmtree(prev_ckpt)  # recursively delete old dir :contentReference[oaicite:2]{index=2}\n\n#     # — Save new checkpoint —\n#     ckpt_dir = f\"./training-ucf/checkpoint_{epoch+1}\"\n#     model.save_pretrained(ckpt_dir)\n#     tokenizer.save_pretrained(ckpt_dir)\n\n#     # — Commit & Push to Hub —\n#     # Copy new checkpoint into the repo root (overwriting previous files)\n#     shutil.copytree(ckpt_dir, \"training-ucf\", dirs_exist_ok=True)\n#     repo.git_add(auto_lfs_track=True)\n#     repo.git_commit(f\"Epoch {epoch+1} — train_loss={avg_train:.4f}, \"\n#                     f\"val_loss={avg_val:.4f}, ROUGE={rouge_scores}\")\n#     repo.git_push()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:32:34.916164Z","iopub.execute_input":"2025-04-20T11:32:34.916408Z","iopub.status.idle":"2025-04-20T11:32:34.921261Z","shell.execute_reply.started":"2025-04-20T11:32:34.916383Z","shell.execute_reply":"2025-04-20T11:32:34.920494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom subprocess import CalledProcessError\nfrom huggingface_hub import Repository\nfrom torch.amp import autocast\nfrom torch.cuda.amp import GradScaler\nimport torch\nimport numpy as np\nimport nltk\nfrom tqdm.auto import tqdm\nimport evaluate\nfrom transformers import VisionEncoderDecoderModel, AutoTokenizer\n\n# ─── Setup Rouge, Model, Tokenizer, Repo ─────────────────────────────────────\nnltk.download(\"punkt\")\nrouge = evaluate.load(\"rouge\")\n\ndef postprocess_text(preds, labels):\n    preds = [\"\\n\".join(nltk.sent_tokenize(p.strip())) for p in preds]\n    labels = [\"\\n\".join(nltk.sent_tokenize(l.strip())) for l in labels]\n    return preds, labels\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    processed_labels = [\n        [(tok if tok != -100 else tokenizer.pad_token_id) for tok in lab]\n        for lab in labels\n    ]\n    decoded_labels = tokenizer.batch_decode(processed_labels, skip_special_tokens=True)\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n    result = rouge.compute(predictions=decoded_preds,\n                           references=decoded_labels,\n                           use_stemmer=True)\n    return {k: round(v * 100, 4) for k, v in result.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:32:34.922308Z","iopub.execute_input":"2025-04-20T11:32:34.922932Z","iopub.status.idle":"2025-04-20T11:32:35.366427Z","shell.execute_reply.started":"2025-04-20T11:32:34.922913Z","shell.execute_reply":"2025-04-20T11:32:35.365653Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\n\n# repo = Repository(local_dir=\"training-ucf\", clone_from=\"username/training-ucf\")\nscaler = GradScaler()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# (Assume train_dataloader, val_dataloader, optimizer, scheduler, epochs are defined…)\n\nfor epoch in range(epochs):\n    # — Training —\n    model.train()\n    train_loss = 0.0\n    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} • Training\"):\n        pixel_values = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"]\n        if labels.dim() == 1:\n            labels = labels.unsqueeze(0)\n        labels = labels.to(device)\n\n        decoder_attention_mask = (labels != tokenizer.pad_token_id).to(device)\n\n        optimizer.zero_grad()\n        with autocast(\"cuda\"):\n            outputs = model(\n                pixel_values=pixel_values,\n                labels=labels,\n                decoder_attention_mask=decoder_attention_mask\n            )\n            loss = outputs.loss\n            train_loss += loss.item()\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n\n    torch.cuda.empty_cache()\n\n    # — Validation & ROUGE —\n    model.eval()\n    val_loss = 0.0\n    all_preds, all_labels = [], []\n    with torch.no_grad():\n        for batch in val_dataloader:\n            pixel_values = batch[\"pixel_values\"].to(device)\n            labels = batch[\"labels\"]\n            if labels.dim() == 1:\n                labels = labels.unsqueeze(0)\n            labels = labels.to(device)\n\n            decoder_attention_mask = (labels != tokenizer.pad_token_id).to(device)\n\n            with autocast(\"cuda\"):\n                outputs = model(\n                    pixel_values=pixel_values,\n                    labels=labels,\n                    decoder_attention_mask=decoder_attention_mask\n                )\n                val_loss += outputs.loss.item()\n\n            generated_ids = model.generate(\n                pixel_values,\n                max_length=labels.shape[-1],\n                num_beams=4,\n                early_stopping=True\n            )\n            all_preds.extend(generated_ids.cpu().tolist())\n            all_labels.extend(labels.cpu().tolist())\n\n    avg_train = train_loss / len(train_dataloader)\n    avg_val   = val_loss   / len(val_dataloader)\n    rouge_scores = compute_metrics((all_preds, all_labels))\n    print(f\"Epoch {epoch+1} — train_loss: {avg_train:.4f}, \"\n          f\"val_loss: {avg_val:.4f}, ROUGE: {rouge_scores}\")\n\n    # — Cleanup old checkpoint locally —\n    prev_ckpt = f\"./training-ucf/checkpoint_{epoch}\"\n    if epoch > 0 and os.path.isdir(prev_ckpt):\n        shutil.rmtree(prev_ckpt)\n\n    # — Save new checkpoint locally —\n    ckpt_dir = f\"./training-ucf/checkpoint_{epoch+1}\"\n    model.save_pretrained(ckpt_dir)\n    tokenizer.save_pretrained(ckpt_dir)\n\n    # — Copy into repo and push (robust) —\n    shutil.copytree(ckpt_dir, \"training-ucf\", dirs_exist_ok=True)\n    repo.git_add(auto_lfs_track=True)\n\n    try:\n        repo.git_commit(\n            f\"Epoch {epoch+1} — train_loss={avg_train:.4f}, \"\n            f\"val_loss={avg_val:.4f}, ROUGE={rouge_scores}\"\n        )\n        repo.git_push()\n    except (CalledProcessError, OSError) as e:\n        # nothing to commit, or other git error → skip without crashing\n        print(f\"[Warning] git commit/push skipped: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T11:32:35.367285Z","iopub.execute_input":"2025-04-20T11:32:35.367500Z","execution_failed":"2025-04-20T11:31:43.383Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save on Huggingface (push)","metadata":{}},{"cell_type":"code","source":"# model_name = \"training-ucf\"\n# hub_repo = f\"NourFakih/{model_name}\"\n\n# # Save locally\n# trainer.save_model(f\"./{model_name}\")\n# tokenizer.save_pretrained(f\"./{model_name}\")\n# feature_extractor.save_pretrained(f\"./{model_name}\")\n# model.save_pretrained(f\"./{model_name}\")\n\n# # Push to Hugging Face Hub\n# tokenizer.push_to_hub(hub_repo)\n# feature_extractor.push_to_hub(hub_repo)\n# model.push_to_hub(hub_repo)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-20T11:31:43.384Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"import os\nimport av\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import AutoImageProcessor, AutoTokenizer, VisionEncoderDecoderModel\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", device)\n\n# Load the models\ndef load_model_and_processor(model_name):\n    processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n    model = VisionEncoderDecoderModel.from_pretrained(model_name).to(device)\n    return processor, tokenizer, model\n\nprocessor1, tokenizer1, model1 = load_model_and_processor(\"Neleac/timesformer-gpt2-video-captioning\")\nprocessor2, tokenizer2, model2 = load_model_and_processor(\"/kaggle/working/training-ucf\")\n\n# Caption generation function\ndef generate_caption(video_path, processor, tokenizer, model, num_frames=16):\n    try:\n        container = av.open(video_path)\n        total_frames = container.streams.video[0].frames\n        indices = set(np.linspace(0, total_frames, num=num_frames, endpoint=False).astype(np.int64))\n        frames = []\n        container.seek(0)\n        for i, frame in enumerate(container.decode(video=0)):\n            if i in indices:\n                frames.append(frame.to_ndarray(format=\"rgb24\"))\n        pixel_values = processor(frames, return_tensors=\"pt\").pixel_values.to(device)\n        tokens = model.generate(pixel_values, min_length=10, max_length=20, num_beams=8)\n        return tokenizer.batch_decode(tokens, skip_special_tokens=True)[0]\n    except Exception as e:\n        print(f\"Error processing {video_path}: {e}\")\n        return \"ERROR\"\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-20T11:31:43.384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load CSV\ncsv_path = \"/kaggle/input/splitted-clips-ucf/splitted_clips_mapping.csv\"\nvideo_folder = \"/kaggle/input/splitted-clips-ucf/\"\n\ndf = pd.read_csv(csv_path)\ndf['video_path'] = df['video_path'].str.replace(r'^\\.\\/splitted_clips\\/', '', regex=True)\ndf=df[100:105]\n# Process each row\ncaptions_1 = []\ncaptions_2 = []\n\nfor idx, row in df.iterrows():\n    video_file = os.path.join(video_folder, row['video_path'])\n    print(f\"[{idx+1}/{len(df)}] Processing {video_file}\")\n\n    cap1 = generate_caption(video_file, processor1, tokenizer1, model1)\n    cap2 = generate_caption(video_file, processor2, tokenizer2, model2)\n\n    captions_1.append(cap1)\n    captions_2.append(cap2)\n\n# Add new columns\ndf['caption_Neleac'] = captions_1\ndf['caption_NourFakih'] = captions_2\n\n# Save to new CSV\noutput_csv_path = \"video_captions_augmented.csv\"\ndf.to_csv(output_csv_path, index=False)\nprint(f\"\\n✅ Captions saved to {output_csv_path}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-20T11:31:43.384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-20T11:31:43.384Z"}},"outputs":[],"execution_count":null}]}