{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**NoteBook Summary**:\n",
    "\n",
    "This notebook demonstrates the evaluation and deployment of a fine-tuned image captioning model (Vit-GPT2-UCA-UCF-06) built on Vision Transformer (ViT) and GPT-2. The model is loaded using Hugging Face Transformers and wrapped in a pipeline for generating natural language captions from input images. A subset of sampled frames from the UCF-Crime dataset is used for inference, where each image is captioned by the model and saved in a new CSV file. To evaluate caption quality, the ROUGE metric is computed across the full dataset and per category, comparing model-generated captions to ground-truth descriptions. Additionally, a visualization function is provided to display test images with their generated captions, enabling quick qualitative inspection of model performance. This notebook supports both quantitative and visual assessment of image-to-text performance in a crime surveillance context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T16:31:34.200332Z",
     "iopub.status.busy": "2025-04-23T16:31:34.200002Z",
     "iopub.status.idle": "2025-04-23T16:32:28.750647Z",
     "shell.execute_reply": "2025-04-23T16:32:28.749522Z",
     "shell.execute_reply.started": "2025-04-23T16:31:34.200306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import VisionEncoderDecoderModel, GPT2Tokenizer, ViTImageProcessor, pipeline\n",
    "\n",
    "# Load the modelcheckpoint-2000\n",
    "model_path= \"NourFakih/Vit-GPT2-UCA-UCF-06\"\n",
    "#model_path= \"NourFakih/Vit-GPT2-UCA-UCF-01\"\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"NourFakih/Vit-GPT2-UCA-UCF-06\")\n",
    "\n",
    "\n",
    "# # # Create the pipeline\n",
    "# image_captioner_1 = pipeline(\n",
    "#     task=\"image-to-text\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     feature_extractor=feature_extractor\n",
    "# )\n",
    "\n",
    "# 1) Check for GPU\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# 2) (Re)create your pipeline on GPU\n",
    "image_captioner_1 = pipeline(\n",
    "    task=\"image-to-text\",\n",
    "    model=model,              # your VisionEncoderDecoderModel\n",
    "    tokenizer=tokenizer,\n",
    "    feature_extractor=feature_extractor,\n",
    "    device=device             # device=0 uses the first GPU :contentReference[oaicite:0]{index=0}\n",
    ")\n",
    "\n",
    "\n",
    "#image_caption_and_display_multiple(image_paths, image_captioner_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T16:33:39.271368Z",
     "iopub.status.busy": "2025-04-23T16:33:39.270985Z",
     "iopub.status.idle": "2025-04-23T16:33:52.306141Z",
     "shell.execute_reply": "2025-04-23T16:33:52.304492Z",
     "shell.execute_reply.started": "2025-04-23T16:33:39.271339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install rouge_score\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:28:01.385655Z",
     "iopub.status.busy": "2025-04-23T11:28:01.385229Z",
     "iopub.status.idle": "2025-04-23T11:28:55.579291Z",
     "shell.execute_reply": "2025-04-23T11:28:55.578293Z",
     "shell.execute_reply.started": "2025-04-23T11:28:01.385611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "csv_path = '/kaggle/input/ucf-crime-extracted-frames/test_image_captions (1).csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "# # Define root directory for the images\n",
    "# root_path = '/kaggle/input/ucf-crime-extracted-frames/'\n",
    "\n",
    "# # Function to update paths\n",
    "# df['image_path'] = root_path + df['image_path']\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df= df[1000:1020]\n",
    "# Function to generate caption for a single image\n",
    "def generate_caption(image_path, caption_pipeline):\n",
    "    try:\n",
    "        # The pipeline returns a list of dictionaries; extract the generated text\n",
    "        result = caption_pipeline(image_path, max_new_tokens=70)\n",
    "        caption = result[0]['generated_text'] if result else \"\"\n",
    "        return caption\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Apply the function on the 'image_path' column and store results in a new column\n",
    "df['gen-caption'] = df['image_path'].apply(lambda x: generate_caption(x, image_captioner_1))\n",
    "\n",
    "# Optionally, save the updated DataFrame to a new CSV file\n",
    "output_csv_path = 'gen-caption.csv'\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print(\"Image caption generation completed and saved to:\", output_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:28:55.580722Z",
     "iopub.status.busy": "2025-04-23T11:28:55.580436Z",
     "iopub.status.idle": "2025-04-23T11:28:55.595686Z",
     "shell.execute_reply": "2025-04-23T11:28:55.594483Z",
     "shell.execute_reply.started": "2025-04-23T11:28:55.580697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Rouge values of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-23T16:38:03.521Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from evaluate import load\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# 1) Load your test CSV\n",
    "df = pd.read_csv(\"/kaggle/input/ucf-crime-extracted-frames/test_image_captions (1).csv\")  \n",
    "# columns: image_path | caption | video_key | category | frame_index\n",
    "\n",
    "# 2) Prepare the metric (ROUGE)\n",
    "metric = load(\"rouge\")  # :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "# 3) Utility to postprocess text for rougeLSum\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [\"\\n\".join(sent_tokenize(p.strip())) for p in preds]\n",
    "    labels = [\"\\n\".join(sent_tokenize(l.strip())) for l in labels]\n",
    "    return preds, labels\n",
    "\n",
    "# 4) Generate captions for each image\n",
    "preds = []\n",
    "refs  = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    img_path = row[\"image_path\"]\n",
    "    gold     = row[\"caption\"]\n",
    "    out      = image_captioner_1(img_path)[0][\"generated_text\"]\n",
    "    preds.append(out)\n",
    "    refs.append(gold)\n",
    "\n",
    "# 5) Compute metrics on the *full* test set\n",
    "p_proc, r_proc = postprocess_text(preds, refs)\n",
    "full_scores = metric.compute(predictions=p_proc, references=r_proc, use_stemmer=True)\n",
    "# Round to percentages\n",
    "full_scores = {k: round(v*100, 4) for k,v in full_scores.items()}\n",
    "\n",
    "print(\"▶ Full Test-Set ROUGE\")\n",
    "print(full_scores)\n",
    "\n",
    "\n",
    "# 6) Compute per-category\n",
    "scores_by_cat = {}\n",
    "for cat, subdf in df.groupby(\"category\"):\n",
    "    idxs = subdf.index.tolist()\n",
    "    p_sub = [preds[i] for i in idxs]\n",
    "    r_sub = [refs[i] for i in idxs]\n",
    "    p_proc, r_proc = postprocess_text(p_sub, r_sub)\n",
    "    cat_scores = metric.compute(predictions=p_proc, references=r_proc, use_stemmer=True)\n",
    "    scores_by_cat[cat] = {k: round(v*100, 4) for k,v in cat_scores.items()}\n",
    "\n",
    "print(\"\\n▶ ROUGE by Category\")\n",
    "for cat, scores in scores_by_cat.items():\n",
    "    print(f\"{cat}: {scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:28:55.597496Z",
     "iopub.status.busy": "2025-04-23T11:28:55.597102Z",
     "iopub.status.idle": "2025-04-23T11:29:22.939023Z",
     "shell.execute_reply": "2025-04-23T11:29:22.937751Z",
     "shell.execute_reply.started": "2025-04-23T11:28:55.597453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, GPT2Tokenizer, ViTFeatureExtractor, pipeline\n",
    "\n",
    "# Define paths\n",
    "image_dir = \"/kaggle/input/test-ucf-uca\"\n",
    "\n",
    "# # Load model components\n",
    "# model_path = \"/kaggle/working/Vit-GPT2-UCA-UCF-04/checkpoint-7686\"\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(model_path)\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "# feature_extractor = ViTFeatureExtractor.from_pretrained(model_path)\n",
    "\n",
    "# Create the image-to-text pipeline\n",
    "# image_captioner_1 = pipeline(\n",
    "#     task=\"image-to-text\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     feature_extractor=feature_extractor\n",
    "# )\n",
    "\n",
    "# Function to caption and display images\n",
    "def image_caption_and_display_multiple(image_dir, captioner):\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.lower().endswith((\"png\", \"jpg\", \"jpeg\"))]\n",
    "\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Generate caption\n",
    "        caption = image_captioner_1( image, max_new_tokens=70)[0]['generated_text']\n",
    "      #  caption = captioner(image)[0][\"generated_text\"]\n",
    "\n",
    "        # Display image with caption\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(caption, fontsize=12, wrap=True)\n",
    "        plt.show()\n",
    "\n",
    "# Apply the function to process images\n",
    "image_caption_and_display_multiple(image_dir, image_captioner_1)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6965269,
     "sourceId": 11274475,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7220397,
     "sourceId": 11518292,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
