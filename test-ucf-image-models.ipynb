{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11274475,"sourceType":"datasetVersion","datasetId":6965269},{"sourceId":11518292,"sourceType":"datasetVersion","datasetId":7220397}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom transformers import VisionEncoderDecoderModel, GPT2Tokenizer, ViTImageProcessor, pipeline\n\n# Load the modelcheckpoint-2000\nmodel_path= \"NourFakih/Vit-GPT2-UCA-UCF-06\"\n#model_path= \"NourFakih/Vit-GPT2-UCA-UCF-01\"\nmodel = VisionEncoderDecoderModel.from_pretrained(model_path)\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)\nfeature_extractor = ViTImageProcessor.from_pretrained(\"NourFakih/Vit-GPT2-UCA-UCF-06\")\n\n\n# # # Create the pipeline\n# image_captioner_1 = pipeline(\n#     task=\"image-to-text\",\n#     model=model,\n#     tokenizer=tokenizer,\n#     feature_extractor=feature_extractor\n# )\n\n# 1) Check for GPU\ndevice = 0 if torch.cuda.is_available() else -1\n\n# 2) (Re)create your pipeline on GPU\nimage_captioner_1 = pipeline(\n    task=\"image-to-text\",\n    model=model,              # your VisionEncoderDecoderModel\n    tokenizer=tokenizer,\n    feature_extractor=feature_extractor,\n    device=device             # device=0 uses the first GPU :contentReference[oaicite:0]{index=0}\n)\n\n\n#image_caption_and_display_multiple(image_paths, image_captioner_1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T16:31:34.200002Z","iopub.execute_input":"2025-04-23T16:31:34.200332Z","iopub.status.idle":"2025-04-23T16:32:28.750647Z","shell.execute_reply.started":"2025-04-23T16:31:34.200306Z","shell.execute_reply":"2025-04-23T16:32:28.749522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install rouge_score\n!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T16:33:39.270985Z","iopub.execute_input":"2025-04-23T16:33:39.271368Z","iopub.status.idle":"2025-04-23T16:33:52.306141Z","shell.execute_reply.started":"2025-04-23T16:33:39.271339Z","shell.execute_reply":"2025-04-23T16:33:52.304492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read the CSV file into a DataFrame\ncsv_path = '/kaggle/input/ucf-crime-extracted-frames/test_image_captions (1).csv'\ndf = pd.read_csv(csv_path)\n# # Define root directory for the images\n# root_path = '/kaggle/input/ucf-crime-extracted-frames/'\n\n# # Function to update paths\n# df['image_path'] = root_path + df['image_path']\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\ndf= df[1000:1020]\n# Function to generate caption for a single image\ndef generate_caption(image_path, caption_pipeline):\n    try:\n        # The pipeline returns a list of dictionaries; extract the generated text\n        result = caption_pipeline(image_path, max_new_tokens=70)\n        caption = result[0]['generated_text'] if result else \"\"\n        return caption\n    except Exception as e:\n        print(f\"Error processing {image_path}: {e}\")\n        return \"\"\n\n# Apply the function on the 'image_path' column and store results in a new column\ndf['gen-caption'] = df['image_path'].apply(lambda x: generate_caption(x, image_captioner_1))\n\n# Optionally, save the updated DataFrame to a new CSV file\noutput_csv_path = 'gen-caption.csv'\ndf.to_csv(output_csv_path, index=False)\n\nprint(\"Image caption generation completed and saved to:\", output_csv_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:28:01.385229Z","iopub.execute_input":"2025-04-23T11:28:01.385655Z","iopub.status.idle":"2025-04-23T11:28:55.579291Z","shell.execute_reply.started":"2025-04-23T11:28:01.385611Z","shell.execute_reply":"2025-04-23T11:28:55.578293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:28:55.580436Z","iopub.execute_input":"2025-04-23T11:28:55.580722Z","iopub.status.idle":"2025-04-23T11:28:55.595686Z","shell.execute_reply.started":"2025-04-23T11:28:55.580697Z","shell.execute_reply":"2025-04-23T11:28:55.594483Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Get the Rouge values of the test set","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm.auto import tqdm\nfrom evaluate import load\nfrom nltk import sent_tokenize\n\n# 1) Load your test CSV\ndf = pd.read_csv(\"/kaggle/input/ucf-crime-extracted-frames/test_image_captions (1).csv\")  \n# columns: image_path | caption | video_key | category | frame_index\n\n# 2) Prepare the metric (ROUGE)\nmetric = load(\"rouge\")  # :contentReference[oaicite:0]{index=0}\n\n# 3) Utility to postprocess text for rougeLSum\ndef postprocess_text(preds, labels):\n    preds = [\"\\n\".join(sent_tokenize(p.strip())) for p in preds]\n    labels = [\"\\n\".join(sent_tokenize(l.strip())) for l in labels]\n    return preds, labels\n\n# 4) Generate captions for each image\npreds = []\nrefs  = []\n\nfor _, row in tqdm(df.iterrows(), total=len(df)):\n    img_path = row[\"image_path\"]\n    gold     = row[\"caption\"]\n    out      = image_captioner_1(img_path)[0][\"generated_text\"]\n    preds.append(out)\n    refs.append(gold)\n\n# 5) Compute metrics on the *full* test set\np_proc, r_proc = postprocess_text(preds, refs)\nfull_scores = metric.compute(predictions=p_proc, references=r_proc, use_stemmer=True)\n# Round to percentages\nfull_scores = {k: round(v*100, 4) for k,v in full_scores.items()}\n\nprint(\"▶ Full Test-Set ROUGE\")\nprint(full_scores)\n\n\n# 6) Compute per-category\nscores_by_cat = {}\nfor cat, subdf in df.groupby(\"category\"):\n    idxs = subdf.index.tolist()\n    p_sub = [preds[i] for i in idxs]\n    r_sub = [refs[i] for i in idxs]\n    p_proc, r_proc = postprocess_text(p_sub, r_sub)\n    cat_scores = metric.compute(predictions=p_proc, references=r_proc, use_stemmer=True)\n    scores_by_cat[cat] = {k: round(v*100, 4) for k,v in cat_scores.items()}\n\nprint(\"\\n▶ ROUGE by Category\")\nfor cat, scores in scores_by_cat.items():\n    print(f\"{cat}: {scores}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-23T16:38:03.521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom transformers import VisionEncoderDecoderModel, GPT2Tokenizer, ViTFeatureExtractor, pipeline\n\n# Define paths\nimage_dir = \"/kaggle/input/test-ucf-uca\"\n\n# # Load model components\n# model_path = \"/kaggle/working/Vit-GPT2-UCA-UCF-04/checkpoint-7686\"\n# model = VisionEncoderDecoderModel.from_pretrained(model_path)\n# tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n# feature_extractor = ViTFeatureExtractor.from_pretrained(model_path)\n\n# Create the image-to-text pipeline\n# image_captioner_1 = pipeline(\n#     task=\"image-to-text\",\n#     model=model,\n#     tokenizer=tokenizer,\n#     feature_extractor=feature_extractor\n# )\n\n# Function to caption and display images\ndef image_caption_and_display_multiple(image_dir, captioner):\n    image_files = [f for f in os.listdir(image_dir) if f.lower().endswith((\"png\", \"jpg\", \"jpeg\"))]\n\n    for image_file in image_files:\n        image_path = os.path.join(image_dir, image_file)\n        image = Image.open(image_path).convert(\"RGB\")\n\n        # Generate caption\n        caption = image_captioner_1( image, max_new_tokens=70)[0]['generated_text']\n      #  caption = captioner(image)[0][\"generated_text\"]\n\n        # Display image with caption\n        plt.figure(figsize=(6, 6))\n        plt.imshow(image)\n        plt.axis(\"off\")\n        plt.title(caption, fontsize=12, wrap=True)\n        plt.show()\n\n# Apply the function to process images\nimage_caption_and_display_multiple(image_dir, image_captioner_1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T11:28:55.597102Z","iopub.execute_input":"2025-04-23T11:28:55.597496Z","iopub.status.idle":"2025-04-23T11:29:22.939023Z","shell.execute_reply.started":"2025-04-23T11:28:55.597453Z","shell.execute_reply":"2025-04-23T11:29:22.937751Z"}},"outputs":[],"execution_count":null}]}